{"cells":[{"cell_type":"code","metadata":{"source_hash":"22307512","execution_start":1730763901957,"execution_millis":788,"execution_context_id":"4f55cacc-7556-49fa-8e62-bde3bb4f347c","cell_id":"792a43cc5a3d47bb9343bd2dfbd3ef42","deepnote_cell_type":"code"},"source":"%matplotlib inline\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch.distributions import Categorical","block_group":"792a43cc5a3d47bb9343bd2dfbd3ef42","execution_count":1,"outputs":[{"name":"stderr","text":"/shared-libs/python3.9/py/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/778cd8c6-956b-4fc7-a812-63742166238f","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"68e79044","execution_start":1730763902797,"execution_millis":69,"execution_context_id":"4f55cacc-7556-49fa-8e62-bde3bb4f347c","cell_id":"d4ee1355aa48477faf40152fc63031d9","deepnote_cell_type":"code"},"source":"# First n-characters to use for training\ndata_size_to_train = \n\n# Load the Sherlock Holmes data up to data_size_to_train\ndata = open(\"/work/Dracula.txt\").read()[:data_size_to_train]\n\n# Find the unique characters within the training data\ncharacters = sorted(list(set(data)))\n\n# total number of characters in the training data and number of unique characters\ndata_size, vocab_size = len(data), len(characters)\n\nprint(\"Data has {} characters, {} unique\".format(data_size, vocab_size))","block_group":"140be837ec7346af9988f729a1f5c8fc","execution_count":2,"outputs":[{"name":"stdout","text":"Data has 5000 characters, 68 unique\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/a3ba65f1-0551-4391-bd77-02a20168a074","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"10ef1fdb","execution_start":1730763902913,"execution_millis":0,"execution_context_id":"4f55cacc-7556-49fa-8e62-bde3bb4f347c","cell_id":"ea992baa418e4613b1830983dc9585fc","deepnote_cell_type":"code"},"source":"# Use Python Dictionary to map the characters to numbers and vice versa\n\ncharacter_to_num = {ch:i for i,ch in enumerate(characters)}\nnum_to_character = {i:ch for i,ch in enumerate(characters)}\n\nprint(character_to_num)","block_group":"45a95b07bc434c83833e824f562229dc","execution_count":3,"outputs":[{"name":"stdout","text":"{'\\n': 0, ' ': 1, '(': 2, ')': 3, ',': 4, '-': 5, '.': 6, '0': 7, '1': 8, '3': 9, '4': 10, '5': 11, '6': 12, '7': 13, '8': 14, ':': 15, ';': 16, '?': 17, 'A': 18, 'B': 19, 'C': 20, 'D': 21, 'E': 22, 'F': 23, 'G': 24, 'H': 25, 'I': 26, 'K': 27, 'L': 28, 'M': 29, 'N': 30, 'O': 31, 'P': 32, 'R': 33, 'S': 34, 'T': 35, 'V': 36, 'W': 37, '_': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64, '’': 65, '“': 66, '”': 67}\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/1f8779ab-2f31-4d7f-a21e-5a9bd6f7af0d","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"2ebfe66c","execution_start":1730763902961,"execution_millis":0,"execution_context_id":"4f55cacc-7556-49fa-8e62-bde3bb4f347c","cell_id":"3e8a3e6bb378495d8dabe6894b66a51d","deepnote_cell_type":"code"},"source":"# Use the character_to_num dictionary to map each character in the training dataset to a number\ndata = list(data)\n\nfor i,ch in enumerate(data):\n    data[i] = character_to_num[ch]\n\nprint(data[:10])","block_group":"a248e9094ac144febd45775a6a86a8b9","execution_count":4,"outputs":[{"name":"stdout","text":"[0, 0, 2, 38, 27, 43, 54, 58, 1, 47]\n","output_type":"stream"}],"outputs_reference":"dbtable:cell_outputs/918413f9-fe10-4474-a33f-fc7b372926f6","content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"ccf76c09","execution_start":1730763903009,"execution_millis":0,"execution_context_id":"4f55cacc-7556-49fa-8e62-bde3bb4f347c","cell_id":"2061a568696843f0ab4ef761f4e02770","deepnote_cell_type":"code"},"source":"class CharRNN(torch.nn.Module):\n    \n    def __init__(self, num_embeddings, embedding_dim, input_size, hidden_size, num_layers, output_size):\n        \n        super(CharRNN, self).__init__()\n        \n        \n    \n    def forward(self, input_seq, hidden_state):\n        \n        \n        return output, hidden_state.detach()","block_group":"9700e635d26347608b62d3e58d6c4727","execution_count":5,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"6494aba6","execution_start":1730763903057,"execution_millis":49,"execution_context_id":"4f55cacc-7556-49fa-8e62-bde3bb4f347c","cell_id":"f93518c7bd9c491bb73c20db6f171216","deepnote_cell_type":"code"},"source":"# Fix random seed\ntorch.manual_seed(25)\n\n# Define RNN network\nrnn = CharRNN(num_embeddings=vocab_size, embedding_dim=___, input_size=____, hidden_size=____, num_layers=___, output_size=vocab_size)\n\n# Define learning rate and epochs\nlearning_rate = ______\nepochs = ______\n\n# Size of the input sequence (Starting chars) to be used during training and validation\ntraining_sequence_len = \nvalidation_sequence_len = \n\n# Define loss function and optimizer\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n\n# add .cuda() for GPU acceleration\nrnn","block_group":"fd38668dc8bd47dba84b4299c093421b","execution_count":6,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"fc75e51b","execution_start":1730763903153,"execution_millis":0,"execution_context_id":"4f55cacc-7556-49fa-8e62-bde3bb4f347c","cell_id":"e4b52502075f4e02a19e97f38ef0b8b4","deepnote_cell_type":"code"},"source":"# Tracking training loss per each input/target sequence fwd/bwd pass\n\ntrain_loss_list = []","block_group":"b72cfedff7c14e02be5a003faa9fdcd6","execution_count":7,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"source_hash":"9cc1a77f","execution_start":1730763903205,"execution_millis":103112,"execution_context_id":"4f55cacc-7556-49fa-8e62-bde3bb4f347c","cell_id":"5b8645fd11574403ab3b96e9d725485d","deepnote_cell_type":"code"},"source":"# Convert training data into torch tensor and make it into vertical orientation (N, 1)\n# Attach .cuda() if using GPU\ndata = torch.unsqueeze(torch.tensor(data), dim=1)\n\n# Training Loop ----------------------------------------------------------------------------------------------------------\n\nfor epoch in range(epochs):\n    \n    character_loc = np.random.randint(100)\n    iteration = 0\n    hidden_state = None\n\n    while character_loc + training_sequence_len + 1 < data_size:\n\n        input_seq = data[character_loc:character_loc+training_sequence_len]\n        target_seq = data[character_loc + 1 : character_loc + training_sequence_len + 1]\n        output, hidden_state = rnn(input_seq, hidden_state)\n        loss = loss_fn(torch.squeeze(output), torch.squeeze(target_seq))\n        train_loss_list.append(loss.item())\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        character_loc += training_sequence_len\n        iteration += 1\n        #print(\"Avg Training Loss for epoch\", epoch, \":\", np.mean(train_loss_list[-iteration:]))\n\n    \n    # Sample and generate a text sequence after every epoch --------------------------------------------------------------\n    \n    character_loc = 0\n    hidden_state = None\n    rand_index = np.random.randint(data_size-1)\n    input_seq = data[rand_index : rand_index+1]\n    print(\"\\n Epoch:\", epoch)\n    print(\"----------------------------------------\")\n    \n    with torch.no_grad():\n        while character_loc < validation_sequence_len:\n            output, hidden_state = rnn(input_seq, hidden_state)\n            output = torch.nn.functional.softmax(torch.squeeze(output), dim=0)\n\n            character_distribution = torch.distributions.Categorical(output)\n            character_num = character_distribution.sample()\n            print(num_to_character[character_num.item()], end='')\n            input_seq[0][0] = character_num.item()\n            character_loc += 1\n\n    print(\"\\n----------------------------------------\")","block_group":"ef8f280c0e22429da8231531e6c3e3ce","execution_count":8,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"execution_context_id":"4f55cacc-7556-49fa-8e62-bde3bb4f347c","deepnote_to_be_reexecuted":true,"cell_id":"21ac485662a44030900daf892b36c71d","deepnote_cell_type":"code"},"source":"# Print a validation text sequence\n\n\ncharacter_loc = 0\nhidden_state = None\nrand_index = np.random.randint(data_size-1)\ninput_seq = data[rand_index : rand_index+1]\n    \nwith torch.no_grad():\n        while character_loc < validation_sequence_len:\n                output, hidden_state = rnn(input_seq, hidden_state)\n                output = torch.nn.functional.softmax(torch.squeeze(output), dim=0)\n\n                character_distribution = torch.distributions.Categorical(output)\n                character_num = character_distribution.sample()\n                print(num_to_character[character_num.item()], end='')\n                input_seq[0][0] = character_num.item()\n                character_loc += 1","block_group":"a4bac2389c924a01978f61112ed1c71e","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"code","metadata":{"execution_context_id":"4f55cacc-7556-49fa-8e62-bde3bb4f347c","deepnote_to_be_reexecuted":true,"cell_id":"13dd8db9ee174783a5b001113bd3091c","deepnote_cell_type":"code"},"source":"# Import seaborn for prettier plot\nimport seaborn as sns\n\nsns.set(style = 'whitegrid', font_scale = 2.5)\n# Plot the training loss and rolling mean training loss with respect to iterations\n# Feel free to change the window size\nplt.figure(figsize = (15, 9))\n\nplt.plot(train_loss_list, linewidth = 3, label = 'Training Loss')\nplt.plot(np.convolve(train_loss_list, np.ones(100), 'valid') / 100, \n         linewidth = 3, label = 'Rolling Averaged Training Loss')\nplt.ylabel(\"training loss\")\nplt.xlabel(\"Iterations\")\nplt.legend()\nsns.despine()","block_group":"7760c30fb8504513987de9006271a6eb","execution_count":null,"outputs":[],"outputs_reference":null,"content_dependencies":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=c13bfaa3-dd3a-4054-b574-b7b25babeb09' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_notebook_id":"0e12fdaa5737449c84f04d9f1689d8b1"}}